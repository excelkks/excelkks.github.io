<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on excelkks</title>
    <link>/post/</link>
    <description>Recent content in Posts on excelkks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Oct 2022 14:01:12 +0800</lastBuildDate><atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>C&#43;&#43;知识</title>
      <link>/post/2022/10/09/c-%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Sun, 09 Oct 2022 14:01:12 +0800</pubDate>
      
      <guid>/post/2022/10/09/c-%E7%9F%A5%E8%AF%86/</guid>
      <description>纯虚类的命名 加if表示interface. 例如HM中的TEncEntropyIf, TComBitIf, TEncBinIf </description>
    </item>
    
    <item>
      <title>vim tips</title>
      <link>/post/2022/10/08/vim-tips/</link>
      <pubDate>Sat, 08 Oct 2022 10:15:05 +0800</pubDate>
      
      <guid>/post/2022/10/08/vim-tips/</guid>
      <description>命令:g和:v :g表示</description>
    </item>
    
    <item>
      <title>Sklearn中的模型</title>
      <link>/post/2022/01/22/sklearn%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sat, 22 Jan 2022 11:06:43 +0800</pubDate>
      
      <guid>/post/2022/01/22/sklearn%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B/</guid>
      <description></description>
    </item>
    
    <item>
      <title>机器学习基础</title>
      <link>/post/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Fri, 21 Jan 2022 10:57:44 +0800</pubDate>
      
      <guid>/post/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</guid>
      <description></description>
    </item>
    
    <item>
      <title>相关系数</title>
      <link>/post/2022/01/20/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</link>
      <pubDate>Thu, 20 Jan 2022 19:01:20 +0800</pubDate>
      
      <guid>/post/2022/01/20/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Python中机器学习相关的基础用法</title>
      <link>/post/2022/01/19/python%E4%B8%AD%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</link>
      <pubDate>Wed, 19 Jan 2022 22:35:56 +0800</pubDate>
      
      <guid>/post/2022/01/19/python%E4%B8%AD%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Norm</title>
      <link>/post/2022/01/19/norm/</link>
      <pubDate>Wed, 19 Jan 2022 16:28:51 +0800</pubDate>
      
      <guid>/post/2022/01/19/norm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>在群晖docker中安装jellyfin</title>
      <link>/post/2021/10/08/%E5%9C%A8%E7%BE%A4%E6%99%96docker%E4%B8%AD%E5%AE%89%E8%A3%85jellyfin/</link>
      <pubDate>Fri, 08 Oct 2021 09:51:28 +0800</pubDate>
      
      <guid>/post/2021/10/08/%E5%9C%A8%E7%BE%A4%E6%99%96docker%E4%B8%AD%E5%AE%89%E8%A3%85jellyfin/</guid>
      <description>参考文章群晖 Docker 安装 Jellyfin 媒体服务器并开启 Intel Quick Sync 提升性能, 以及解决 Docker 安装 Jellyfin 封面图和部分中文字幕变方块</description>
    </item>
    
    <item>
      <title>将群晖上的http服务反向代理为https</title>
      <link>/post/2021/10/08/%E5%B0%86%E7%BE%A4%E6%99%96%E4%B8%8A%E7%9A%84http%E6%9C%8D%E5%8A%A1%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E4%B8%BAhttps/</link>
      <pubDate>Fri, 08 Oct 2021 09:05:51 +0800</pubDate>
      
      <guid>/post/2021/10/08/%E5%B0%86%E7%BE%A4%E6%99%96%E4%B8%8A%E7%9A%84http%E6%9C%8D%E5%8A%A1%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E4%B8%BAhttps/</guid>
      <description>在文章给群晖添加ssl证书中已经讲诉了如何添加ssl证书, 这对群晖默认端口的服务是直接生效的, 但是对其他如docker等的服务是无效的, 对此, 只需要利用群晖自带的反向代理就可以完成ssl加密. 需要完成反向代理和路由器端口映射. 本文以设置jellyfin反向代理为例. ## 前提
假设已经架设好jellyfin服务了, 我使用的是docker中jellyfin服务, 也可以是其他方式, 主要目的是架设好jellyfin服务. 假设架设好的jellyfin服务后的端口号为8096, 此时已经可以通过8096端口访问jellyfin的http服务了, 以上为前提条件.
反向代理 打开群晖的控制面板-&amp;gt;应用程序门户-&amp;gt;反向代理服务规则, 像下图一样填写, 表示将来自于https的18096端口的服务代理到http的8096端口, 完成后, 即可通过https的18096访问jellyfin了, 但是此时还未添加证书.
为反向代理添加证书 打开群晖的控制面板-&amp;gt;安全性-&amp;gt;证书-&amp;gt;配置, 如下图, 给10896端口的服务选择你的证书.
此时, 已经通过ssl加密了. 下一步需要在路由器上设置防火墙.
防火墙设置 在防火墙中放行18096端口即可.</description>
    </item>
    
    <item>
      <title>给群晖添加ssl证书</title>
      <link>/post/2021/10/05/%E7%BB%99%E7%BE%A4%E6%99%96%E6%B7%BB%E5%8A%A0ssl%E8%AF%81%E4%B9%A6/</link>
      <pubDate>Tue, 05 Oct 2021 08:14:44 +0800</pubDate>
      
      <guid>/post/2021/10/05/%E7%BB%99%E7%BE%A4%E6%99%96%E6%B7%BB%E5%8A%A0ssl%E8%AF%81%E4%B9%A6/</guid>
      <description>acme.sh已经把申请默认证书从Let&amp;rsquo;s Encrypt改为了ZeroSSL了. 本文利用acme.sh给群晖申请ZeroSSL证书, 并自动续期, 步骤包括:
获取DNS的API用以后续验证域名属于你. 申请zerossl帐户. 利用acme.sh申请证书. 安装证书 自动续期证书. 获取DNS服务商的API 在申请证书的时候需要验证DNS属于是属于你的, 所以要先获取dns服务商的API以便在利用acme.sh申请时自动验证. 我这里使用的是godaddy的域名服务商自带的dns, 所以这里以godaddy举例.
首先打开godaddy的开发者网页, 点击API Keys
根据提示申请一个Production的API Key
之后获得Key和Secret
其他服务商获取方法大同小异.
申请ZeroSSL帐户 打开ZeroSSL官网, 根据提示申请帐户, 这里需要提供一个邮箱, 这个邮箱在acme.sh申请证书时需要用到. 这里多说一句, 利用ZeroSSL官网就已经可以通过图形界面来申请证书了, 并不一定需要acme.sh, 但是我们可以利用acme.sh来自动化续签, 所以还是采用acme.sh的方法来申请证书.
acme.sh申请证书 首先下载acme.sh
# 下载并解压acme.sh wget https://github.com/acmesh-official/acme.sh/archive/master.tar.gz tar xvf master.tar.gz cd acme.sh-master/ chmod a+x acme.sh 在acme.sh的目录下, 有一个dnsapi的目录, 里面存放的是修改各个DNS服务商的API的样本文件, 我这里使用的godaddy, 所以编辑dnsapi/dns_gd.sh, 修改GD_Key和GD_Secret为之前申请的值.
#!/usr/bin/env sh #Godaddy domain api # #GD_Key=&amp;#34;这里修改为申请到的GD_Key&amp;#34; # #GD_Secret=&amp;#34;这里修改为申请到的GD_Secret&amp;#34; GD_Api=&amp;#34;https://api.godaddy.com/v1&amp;#34; 申请证书, --email参数为申请ZeroSSL时所用的邮箱, --dns为dns服务商
./acme.sh --email example@email.com --issue -d example.</description>
    </item>
    
    <item>
      <title>直接初始化和拷贝初始化</title>
      <link>/post/2021/03/02/%E7%9B%B4%E6%8E%A5%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%8B%B7%E8%B4%9D%E5%88%9D%E5%A7%8B%E5%8C%96/</link>
      <pubDate>Tue, 02 Mar 2021 13:48:37 +0800</pubDate>
      
      <guid>/post/2021/03/02/%E7%9B%B4%E6%8E%A5%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%8B%B7%E8%B4%9D%E5%88%9D%E5%A7%8B%E5%8C%96/</guid>
      <description>拷贝初始化底层是先调用构造函数创建一个临时变量，再将临时变量通过拷贝构造函数创建新变量。考虑如下过程
class A{ public: A() { cout &amp;lt;&amp;lt; &amp;#34;default constructor&amp;#34; &amp;lt;&amp;lt; endl; } A(const string&amp;amp; _str):str(_str) { cout &amp;lt;&amp;lt; &amp;#34;constructor&amp;#34; &amp;lt;&amp;lt; endl; } explicit A(const A&amp;amp; a) { cout &amp;lt;&amp;lt; &amp;#34;copy constructor&amp;#34; &amp;lt;&amp;lt; endl; } string str; } int main() { A a1; // direct initialiazation A a2(string(&amp;#34;hi&amp;#34;)); // direct initialiazation A a3 = string(&amp;#34;hi&amp;#34;); // error, string(&amp;#34;hi&amp;#34;) can&amp;#39;t // implicit convert to A // (copy initialiazation) A a4 = a2; // the same as above // a2 can&amp;#39;t implicit convert to A // by call copy constructor } 拷贝初始化发生情况 用= 定义变量时 string s1(&amp;#34;hi&amp;#34;); // direct initialiazation string s2 = s1; // copy initialiazation 将一个对象作为实参传递给一个非引用类型的形参 void Foo(string s) { } Foo(s1); // copy initialiazation 从一个返回类型为非引用类型的函数返回一个对象 string Foo2(){ string s(&amp;#34;hi&amp;#34;); return s; } Foo2(); // copy initialiazation 用花括号列表初始化一个数组中的元素或一个聚合类中的成员 // copy initialiazation string strs[] = {&amp;#34;This&amp;#34;, &amp;#34;is&amp;#34;, &amp;#34;copy&amp;#34;, &amp;#34;initialiazation&amp;#34;}; struct Data { int x; int y; }; // copy initialiazation Data dt = {0, 0}; 三五法则 拷贝构造函数 拷贝赋值运算符 析构函数 移动构造函数 移动赋值运算符 需要析构函数的类也需要拷贝和赋值操作</description>
    </item>
    
    <item>
      <title>在linux中开启samba服务</title>
      <link>/post/2021/01/06/%E5%9C%A8linux%E4%B8%AD%E5%BC%80%E5%90%AFsamba%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Wed, 06 Jan 2021 14:32:55 +0800</pubDate>
      
      <guid>/post/2021/01/06/%E5%9C%A8linux%E4%B8%AD%E5%BC%80%E5%90%AFsamba%E6%9C%8D%E5%8A%A1/</guid>
      <description>linux中samba的应用 服务器端 在linux中开启samba共享服务，首先安装samba服务
# ubuntu apt install samba 配置文件的路径在/etc/samba/smb.conf，如果需要添加共享，需要添加如下内容
[share] comment = comments path = /path/to/share browseable = yes writeable = yes public = yes # create mask = 0775 # 创建文件的权限 # directory mask = 0775 # 创建目录的权限 # valid users = heijunma,root #允许访问该共享的用户 # write list = heijunma,root #可写入共享的用户列表 重启smb服务
service smbd restart service nmb restart 在linux中访问samba共享 首先需要安装cifs，再进行挂载，samba的默认端口为445和139
apt install cifs-utils mount.cifs -o username=user,password=passwd,port=&amp;lt;port&amp;gt; //ip/share/path/ /mount/point </description>
    </item>
    
    <item>
      <title>omv搭建一</title>
      <link>/post/2021/01/06/omv%E6%90%AD%E5%BB%BA%E4%B8%80/</link>
      <pubDate>Wed, 06 Jan 2021 14:21:43 +0800</pubDate>
      
      <guid>/post/2021/01/06/omv%E6%90%AD%E5%BB%BA%E4%B8%80/</guid>
      <description>omv配置网络 在面板上设置了静态网络发现无法上网，因此这里直接编辑netplan的配置文件实现静态ipv4.
编辑文件/etc/netplan/10-openmediavault-eno1.yaml
network: ethernets: eno1: match: macaddress: xx:xx:xx:xx:xx:xx dhcp4: false dhcp6: false addresses: [192.168.2.222/24] gateway4: 192.168.2.1 nameservers: addresses: [114.114.114.114, 8.8.8.8] link-local: [] 最后执行
netplan apply 上面编辑的配置文件中相当于没有启用ipv6，如果要启用ipv6，可以把 dhcp6: false 改为dhcp6: true .
docker安装 wget -O - https://github.com/OpenMediaVault-Plugin-Developers/packages/raw/master/install | bash ipv6 ddns 首先要有一个域名，可以使用常见的阿里云域名、cloudflare 或者是godaddy。由于我之前已经有了一个godaddy的域名，我这里直接用godaddy。关于阿里云域名的教程在网上很多，可以更容易解决。
关于申请域名部分比较简单，这里不赘述。
增加AAAA记录 打开管理域名页面.
创建API Key 进入管理API页面，创建api key
Next后得到 Key和Secret.
创建脚本 将上面的到的Key和Secret填入下方
#!/bin/bash #这里是你购买的域名 mydomain=&amp;#34;domain.xyz&amp;#34; #这里是dns配置中的名称 myhostname=&amp;#34;router&amp;#34; #这里key和Secret之间注意有个冒号 gdapikey=&amp;#34;你的key:你的Secret&amp;#34; logdest=&amp;#34;local7.info&amp;#34; #另外注意，我这里的ipv6地址，所以使用的是AAAA类型解析，如果是ipv4那么下面所有的AAAA需改为A setNewIp(){ #这里的地址也是为ipv6服务，ipv4地址为https://api.ipify.org myip=`curl -s &amp;#34;https://api6.ipify.org&amp;#34;` dnsdata=`curl -s -X GET -H &amp;#34;Authorization: sso-key ${gdapikey}&amp;#34; &amp;#34;https://api.</description>
    </item>
    
    <item>
      <title>systemd开机启动服务</title>
      <link>/post/2021/01/06/systemd%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Wed, 06 Jan 2021 14:18:40 +0800</pubDate>
      
      <guid>/post/2021/01/06/systemd%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1/</guid>
      <description>systemd开机运行服务 [Unit] Description=renew ipv6 (ddns) After=network.target # needed service [Service] Type=oneshot # start once ExecStart=/bin/bash /etc/scripts # what to execute [Install] WantedBy=multi-user.target # startup execute 参考How to write startup script for systemd</description>
    </item>
    
    <item>
      <title>Ssh黑名单</title>
      <link>/post/2021/01/06/ssh%E9%BB%91%E5%90%8D%E5%8D%95/</link>
      <pubDate>Wed, 06 Jan 2021 14:16:18 +0800</pubDate>
      
      <guid>/post/2021/01/06/ssh%E9%BB%91%E5%90%8D%E5%8D%95/</guid>
      <description>linux 服务器防止ssh暴力登录 最近折腾nas的时候意识到linux服务器安全性问题，这里主要是防止利用ssh暴力登录。ssh登录记录一般在/var/log/auth.log(debian系)或者/var/log/secure(centos系)中，可以通过这个文件查看ssh的登录记录，包括登录失败，拒绝登录，登录成功等信息。
参看登录失败IP
cat /var/log/auth.log | awk &amp;#39;/Failed/{print $(NF-3)}&amp;#39; | sort | uniq -c | awk &amp;#39;{print $2&amp;#34; = &amp;#34;$1;}&amp;#39; 参看登录成功IP
cat /var/log/auth.log | awk &amp;#39;/Accepted/{print $(NF-3)}&amp;#39; | sort | uniq -c | awk &amp;#39;{print $2&amp;#34; = &amp;#34;$1;}&amp;#39; 参考拒绝登录IP
cat /var/log/auth.log | awk &amp;#39;/refused/{print $(NF -1)}&amp;#39; | sort |uniq -c | awk &amp;#39;{print $2&amp;#34; = &amp;#34;$1}&amp;#39; 加入黑名单 将3次登录失败的IP加入/etc/hostd.deny黑名单，禁止其通过sshd登录。这里每10分钟检查一次log文件。脚本文件/scripts/host_block.sh如下：
#!/bin/bash # -*- coding: UTF-8 -*- # Filename: host_block.sh # Description: 将SSH多次登录失败的IP加入黑名单 # Date: 2021-01-06 block_ip(){ cat /var/log/auth.</description>
    </item>
    
    <item>
      <title>交叉熵损失函数</title>
      <link>/post/2020/12/18/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
      <pubDate>Fri, 18 Dec 2020 11:50:44 +0800</pubDate>
      
      <guid>/post/2020/12/18/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</guid>
      <description>交叉熵损失函数 信息量 根据字面意思，信息量就是事件携带的信息量。比如你们宿舍有1个人大家都认为他几乎不可能找到女朋友，你们说他90%的概率是找不到女朋友的，那么他找到女朋友的概率为10%，他每天回到宿舍都对你们说，今天没有找到女朋友，此时的你们的内心毫无波澜，因为这在你们看来这几乎是板上钉钉的事(他90%的概率找不到女朋友)，他找不到女朋友这件事几乎没啥信息量。但是有一天，他回到宿舍突然告诉你们有女孩接受了他的表白，你们宿舍的人惊得把下巴都掉了，这们会惊呼“这信息量太大了”，并纷纷向他来难以置信的目光。可见，概率越小的事件携带信息量越大，这就是信息量的通俗解释，可以用定义为 $$ I = -\log_2(p) $$ 假设你的室友找不到女朋友记作$p=0.9$，可以找到女朋友则是$\hat{p} = 1-0.9=0.1$，那么，你室友找不到女朋友的信息量为$-\log_2(p)=0.15200$，找得到女朋友的信息量为$-\log_2(\hat{p})=3.32193$
信息熵 熵表述的是系统的混乱程度，在这里用信息量的期望来描述，一个包含$n$种可能性的事件的熵为 $$ H = \sum\limits_{i=1}^n-p_i\log_2p_i = \sum\limits_{i=1}^np_i\log_2(\frac{1}{p_i}) $$ 在上面这个例子中，如果你的室友是否找到女朋友的概率都是50%，那么他是否找到女朋友的概率是一样的（也就是信息量是一样的），所有这件事是很整齐的，表现得一点也不混乱，信息熵为$0.5\log_20.5 + 0.5\log_20.5=1$。如果概率分别为90%，10%，则信息熵为$0.9*\log_20.9+0.1*\log_20.1 = 0.468$，这就说明信息量很混乱，信息熵很小。
相对熵 相对熵也称为KL散度，表述的是同一个随机变量$X$的不同概率分布$P(X)$和$Q(X)$的相似程度，相对熵越小，表示分布越相近。相对熵计算方法为： $$ D_{KL}(p||q) = \sum\limits_{i=1}^np_i\log{\frac{p_i}{q_i}} $$ 例如你的室友A找到女朋友的概率为$p=0.1$，室友B找到女朋友的概率为$q=0.2$。那么，相对熵为 $$ 0.1\log_2\frac{0.1}{0.2}+0.9\log_2\frac{0.9}{0.8}=0.0529 $$
交叉熵 铺垫了这么多，终于讲到交叉熵了。将相对熵$D_{KL}$稍微变形一下： $$ D_{KL}(p||q)=\sum\limits_{i=1}^np_i\log\frac{p_i}{q_i}=\sum\limits_{i=1}^np_i\log p_i - \sum\limits_{i = 1}^np_i\log q_i $$ 如果我们的关注点只在q和b的相似程度，由于前部分$\sum\limits_{i=1}^np_i\log p_i$是恒定的，只需要关注后部分$-\sum\limits_{i=1}^np_i\log q_i$，这部分便是交叉熵，可以用于描述分布$q$离分布$p$的距离。因此在机器学习中的交叉熵损失函数便是用于描述lable和prediction之间的差距。 $$ H(p,q)=\sum\limits_{i=1}^np_i\log \frac{1}{q_i} $$</description>
    </item>
    
    <item>
      <title>Projection</title>
      <link>/post/2020/12/18/projection/</link>
      <pubDate>Fri, 18 Dec 2020 11:45:23 +0800</pubDate>
      
      <guid>/post/2020/12/18/projection/</guid>
      <description>projection $p = \hat{x}a$
$e = b - \hat{x}a$
$a\perp e \to a^T(b-\hat{x}a)=a^Tb - \hat{x}a^Ta = 0 \to \hat{x}=\frac{a^Tb}{a^Ta}$
$p = \hat{x}a = a\frac{a^Tb}{a^Ta}$
$p = Pb = \frac{aa^T}{a^Ta}b \to P = \frac{aa^T}{a^Ta}$
$P = P^T$, $P = P^2$
$P$ is a line trough $a$.
why projection? Because $Ax = b$ may have no solution. So we may find a $p$ closest to $b$ that makes $A\hat{x} = p$ do have solution.</description>
    </item>
    
    <item>
      <title>截断正态分布</title>
      <link>/post/2020/12/18/%E6%88%AA%E6%96%AD%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/</link>
      <pubDate>Fri, 18 Dec 2020 11:41:55 +0800</pubDate>
      
      <guid>/post/2020/12/18/%E6%88%AA%E6%96%AD%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/</guid>
      <description>截断正态分布 要理解截断正态分布(truncated normal distribution)，需要先理解截断分布(truncated distribution)。所谓截断分布就是对原本分布中随机变量进行前后截断取值，在随机变量取值被改变的情况下，概率分布需要根据阶段的概率分布重新归一化到$[0,1]$。假设$X$的概率密度函数为$f(x)$，累计概率分布为$F(x)$，如果随机变量$X$的取值截断为$[a,b]$，那么，截断后的概率密度函数应该为 $$ g(x;a,b) = \frac{f(x)}{F(b)-F(a)} $$ 接下来，截断正态分布就很好理解了，假设$X\sim N(\mu, \delta)$，则$X$的概率密度函数为$\frac{1}{\delta}\phi(x)$，累计概率分布函数为$\Phi(x)$，那么，$X$截断为$[a,b]$的概率密度分布函数为 $$ g(x;\mu,\delta,a,b) = \frac{\frac{1}{\delta}\phi(\frac{x-\mu}{\delta })}{\Phi(\frac{b-\mu}{\delta})-\Phi(\frac{a-\mu}{\delta})} $$ 其中，$\phi(x)$，$\Phi(x)$分别为标准正态分布的概率密度函数和累积概率函数 $$ \phi(x) =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2} $$
$$ \Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-\frac{-t^2}{2}}dt $$</description>
    </item>
    
    <item>
      <title>Pytorch Sequential</title>
      <link>/post/2020/12/18/pytorch-sequential/</link>
      <pubDate>Fri, 18 Dec 2020 11:39:33 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch-sequential/</guid>
      <description>torch.nn.Sequential 快速构建一个neural network可以通过torch.nn.Module构建类，也可通过设计一个torch.nn.Sequential来构建网络计算过程。实际上，只需要将网络上的各个计算过程都堆叠在Sequential中既可以了，例如：
model = torch.nn.Sequential( torch.nn.Linear(3, 1), torch.nn.Flatten(0,1)) Sequential是一个容器，一般来说，可以用多个Sequential来构建Module. 例如LeNet-5网络的构建
class LeNet5(torch.nn.Module): def __init__(self): super(LeNet5,self).__init__() self.conv1 = torch.nn.Sequential( torch.nn.Conv2d(1,6,5), torch.nn.ReLU(), torch.nn.MaxPool2d(2,2) ) self.conv2 = torch.nn.Sequential( torch.nn.Conv2d(6,16,5), torch.nn.ReLU(), torch.nn.MaxPool2d(2,2) ) self.conv3 = torch.nn.Sequential( torch.nn.Conv2d(16,120,5), torch.nn.ReLU() ) self.fc = torch.nn.Sequential( torch.nn.Linear(120,84), torch.nn.ReLU(), torch.nn.Linear(84,10), torch.nn.LogSoftmax(dim=-1) ) def forward(self, image): output = self.conv1(image) output = self.conv2(output) output = self.conv3(output) output = output.view(image.size(0),-1) output = self.fc(output) return output </description>
    </item>
    
    <item>
      <title>Pytorch nn-Module</title>
      <link>/post/2020/12/18/pytorch-nn-module/</link>
      <pubDate>Fri, 18 Dec 2020 11:35:09 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch-nn-module/</guid>
      <description>[toc]
nn.Module 继承nn.Module类 在nn(neural network)模块中，有一个nn.Module类，这是一个快速构建神经网络的基类。你可以通过继承这个类，并重写forward(input)方法，来创建一个自定义各个网络层的神经网络类。例如下面这个Net	类定义了一个LeNet-5网络。
import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 3x3 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6*6 from image dimension self.fc2 = nn.Linear(120, 84) self.</description>
    </item>
    
    <item>
      <title>Pytorch自定义函数</title>
      <link>/post/2020/12/18/pytorch%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/</link>
      <pubDate>Fri, 18 Dec 2020 11:33:36 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/</guid>
      <description>[toc]
torch.autograd.Function 自定义一个计算过程，并且定义其反向传播过程。例如计算 $y = a+bx+cx^2+dx^3$时，用两步替代该过程 $y= a+b\times P_3(c+dx), P_3(x) = \frac{1}{2}(5x^3-3x)$
那么可以通过torch.autograd.Function定义$P_3(x)$，过程如下：
# -*- coding: utf-8 -*- import torch import math class LegendrePolynomial3(torch.autograd.Function): &amp;#34;&amp;#34;&amp;#34; We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. &amp;#34;&amp;#34;&amp;#34; @staticmethod def forward(ctx, input): &amp;#34;&amp;#34;&amp;#34; In the forward pass we receive a Tensor containing the input and return a Tensor containing the output.</description>
    </item>
    
    <item>
      <title>Pytorch基础</title>
      <link>/post/2020/12/18/pytorch%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Fri, 18 Dec 2020 11:30:07 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch%E5%9F%BA%E7%A1%80/</guid>
      <description>[toc]
Tensor的创建 tensor是基本的数据类型，基本的创建方式有
torch.empty(5, 3) 未初始化的tensor, 里面的值为内存中原有的值 torch.randn(5,3) 创建tensor，并随机初始化 torch.zeros(5, 3, dtype=torch.long) 创建tensor，并初始化为0，指定数据类型 torch.tensor([5.5, 3])创建tensor，并直接值初始化 除了上述的创建方式，还有可以利用已经存在的tensor来创建，假设现有x = torch.zeros(5,3, dtype=torch.float32)，那么，可以根据以下方式创建
x.new_ones(5,3) 创建tensor，并值初始化为0，除了shape外，其他属性与x一致 torch.randn_like(x, dtype=torch.float) 创建tensor，随机初始化值，除了明确指定的属性type=torch.float外，其他属性与x一致 Operations 加法 直接加
x = torch.randn(5, 3) y = torch.randn(5, 3) # 直接加 result1 = x + y 函数方法
# 结果赋值到result2 result2 = torch.add(x, y) # 预先指定结果保存到变量 torch.add(x, y, out=result3) 立地加(in-place)
# 立地(in-place)加，结果直接加到被加数中, 一般立地加的函数都以_结束， # 例如x.copy_(y), x.t_() y.add_(x) 索引 可以使用标准的类似numpy的索引。例如x[:, 1]
resize resize或者reshape一个tensor，可以使用函数torch.view，例如
x = torch.randn(4, 4) y = x.</description>
    </item>
    
    <item>
      <title>os模块</title>
      <link>/post/2020/12/11/os%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Fri, 11 Dec 2020 10:49:06 +0800</pubDate>
      
      <guid>/post/2020/12/11/os%E6%A8%A1%E5%9D%97/</guid>
      <description>os.walk os.walk(path)用于遍历目录。返回值是一个遍历器，需要用for循环不断遍历，每次遍历得到长度为3的tuple，分别为(目录, [目录下的子目录], [目录下的文件])。 使用方法示例:
for root, dirs, files in os.walk(&amp;#39;.&amp;#39;): for file in files: print(os.path.join(root, file)) for dir in dirs: print(os.path.join(root, dir)) ./directory_2 ./directory_1 ./directory_2/file_2_1 ./directory_2/directory_2_1 ./directory_2/directory_2_1/file_2_1_1 ./directory_1/directory_1_1 ./directory_1/directory_1_2 ./directory_1/directory_1_1/file_1_1_2 ./directory_1/directory_1_1/file_1_1_1 ./directory_1/directory_1_2/file_1_2_1 glob模块 glob.glob glob.glob用于以shell-style的方式匹配目录下的文件, 例如shell匹配所有jpg图片
*.jpg 在python中用glob.glob的方式为
files = glob.glob(&amp;#39;*.py&amp;#39;) numpy模块 random.get_state numpy.random.get_state()一般与numpy.random.set_state()和numpy.random.shuffle()一起使用，用于打乱numpy数组的顺序。例如打乱数据集
state = np.random.get_state() np.random.shuffle(dataset_images) # 保证打乱的顺序与之前一致 np.random.set_state(state) np.random.shuffle(dataset_labels) </description>
    </item>
    
    <item>
      <title>Jupyter Notebook</title>
      <link>/post/2020/12/09/jupyter-notebook/</link>
      <pubDate>Wed, 09 Dec 2020 10:00:18 +0800</pubDate>
      
      <guid>/post/2020/12/09/jupyter-notebook/</guid>
      <description>在本地使用服务器上的jupyter-notebook
在服务器端开启 jupyter notebook --no-browser --port=9009 在本地端 ssh -N -f -L localhost:9008:localhost:9009 user@server-ip </description>
    </item>
    
    <item>
      <title>Tensorflow</title>
      <link>/post/2020/12/09/tensorflow/</link>
      <pubDate>Wed, 09 Dec 2020 09:58:47 +0800</pubDate>
      
      <guid>/post/2020/12/09/tensorflow/</guid>
      <description>使用GPU 为了避免占用全部GPU，需要加上
import os os.environ[&amp;#39;CUDA_DEVICES_ORDER&amp;#39;] = &amp;#39;PCI_BUS_ID&amp;#39; os.environ[&amp;#39;CUDA_VISIBLE_DEVICES&amp;#39;] = &amp;#39;0&amp;#39; config = tf.ConfigProto() #config.gpu_options.per_process_gpu_memory_fraction = 0.9 config.gpu_options.allow_growth = True tf.truncated_normal_initializer
生成截断正态分布的初始化器类
参数 mean: 分布的均值 stddev: standard deviation，标准差 seed: 随机种子 dtype: 数据类型，只能是float的类型，可选位数 tf.get_variable()
获取一个变量或者创建一个新的变量
重要参数 shape: list类型，是指变量的shape Initializer: 用于生成变量的初始化器 tf.nn.conv2d()
2维卷积函数，列举主要参数tf.nn.conv2d(input, filter=None, strides=None, paddig=None, data_format=&amp;lsquo;NHWC&amp;rsquo;)。
重要参数 input: 输入4-D的Tensor，数据格式只能是half,bfloat16,float32,float64 filter: 卷积核，也是一个4-D tensor，数据格式必须与input一样 strides: 移动步长，可以是长度为1,2,4的list，长度为1时，表示宽高方向的移动步长，并且一致。长度为2时，分别指定宽高方向的移动步长。长度为4时，设定根据data_format指定的位置对应方向移动步长。 padding: &amp;lsquo;VALID&amp;rsquo;表示不填充， SAME&amp;rsquo;表示填充。填充方式根据filter而定，如果filter的一个维度上的长度为奇数，则在该维度起始处和结束处填充0，如果为偶数则在结束处填充0。 一般来说，input的各个维度为[batch, in_height, in_width, in_channels]，filter的各个维度为 [filter_height, filter_width, in_channels, out_channels]
初始化变量 变量需要初始化，初始化方法有单独初始化和全局初始化。
单独初始化 v1 = tf.Variable(tf.random_normal([2, 3], stddev=2)) v2 = tf.</description>
    </item>
    
    <item>
      <title>深度学习</title>
      <link>/post/2020/12/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Thu, 03 Dec 2020 16:47:55 +0800</pubDate>
      
      <guid>/post/2020/12/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</guid>
      <description>1. 避免tensorflow占用全部GPU 使用tensorflow时，启用GPU支持会占用所有的GPU，虽然每个GPU都只占用了一点，GPU占用情况可以用如下命令查看：
nvidia-msi 解决GPU全占的办法是加入以下代码
os.environ[&amp;#39;CUDA_DEVICES_ORDER&amp;#39;] = &amp;#39;PCI_BUS_ID&amp;#39; os.environ[&amp;#39;CUDA_VISIBLE_DEVICES&amp;#39;] = &amp;#39;0&amp;#39; # 使用 0 GPU config = tf.ConfigProto() #config.gpu_options.per_process_gpu_memory_fraction = 0.9 # 控制使用每个GPU的大小 config.gpu_options.allow_growth = True </description>
    </item>
    
    <item>
      <title>修复latex公式矩阵换行在hugo下的渲染问题</title>
      <link>/post/2020/12/03/%E4%BF%AE%E5%A4%8Dlatex%E5%85%AC%E5%BC%8F%E7%9F%A9%E9%98%B5%E6%8D%A2%E8%A1%8C%E5%9C%A8hugo%E4%B8%8B%E7%9A%84%E6%B8%B2%E6%9F%93%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 03 Dec 2020 14:13:46 +0800</pubDate>
      
      <guid>/post/2020/12/03/%E4%BF%AE%E5%A4%8Dlatex%E5%85%AC%E5%BC%8F%E7%9F%A9%E9%98%B5%E6%8D%A2%E8%A1%8C%E5%9C%A8hugo%E4%B8%8B%E7%9A%84%E6%B8%B2%E6%9F%93%E9%97%AE%E9%A2%98/</guid>
      <description>目前我已经从hexo迁移到hugo写博客，我个人非常喜欢hugo的轻快，但是还有一个非常困扰我的问题就是latex矩阵的渲染。我在hugo使用的渲染公式的方案是用mathjax，由于hugo解析backslash\先用mathjax，这就会导致公式中的double backslash\\（在latex中表现为换行）会被解析为single backslash，从而导致mathjax解析换行失败，最后造成无法渲染矩阵。
在网上搜了一圈后，普遍给出的方案都是把 double backslash 换为quadruple backslash\\\\，这样确实可以解决问题，但是问题也很明显，就是无法与其他的 Markdown 编辑器兼容，特别是我平时喜欢用 Tyopra 编辑markdown文件。
我的最终方案是在hugo博客的根目录下建立一个data路径用于存放原始的markdown文件，平时的编辑，更改都在data/post路径下的mardown文件进行，写好后再用脚本将data/post文章的公式中的double backslash改为quadraple backslash后放入content/post中。这样平时只需要在data/post中编辑，编辑完后在用脚本renew一下就可以了。这应该是我能想到的最不折腾的方案了。
我的写作流程基本上可以归结为如下：
blog -n &amp;quot;post title&amp;quot; 基于hugo的hugo new post/post_title 创建post并放到data/post路径下 在data/post路径下写作 blog -r 更新所有post，即处理\\ blog -d &amp;quot;comment&amp;quot; 提交到仓库，更新博客网站 </description>
    </item>
    
    <item>
      <title>LaTeX的设置</title>
      <link>/post/2020/10/28/latex%E7%9A%84%E8%AE%BE%E7%BD%AE/</link>
      <pubDate>Wed, 28 Oct 2020 11:02:29 +0800</pubDate>
      
      <guid>/post/2020/10/28/latex%E7%9A%84%E8%AE%BE%E7%BD%AE/</guid>
      <description>最近思来想去，还是决定用LaTeX来写文档. 大概复习了一下基本的用法, 首先需要解决的就是中文支持的问题, 目前比较推荐的方式是用CTEX, 支持的documentclass有4种，分别是
ctexart 对应artical
ctexrep 对应report
ctexbook对应book
ctexbeamer对应breamer
以下是一个基本示例
\documentclass[UTF8]{ctexart} \begin{document} 中文文档类测试。你需要将所有的源文件保存为 UTF-8 编码。 你可以使用 XeLaTeX 或 LuaLaTeX 编译。 \end{document} 具体的可以查看宏包的文档, 查看宏包文档的方式是在终端输入texdoc &amp;lt;package&amp;gt;, 这里输入texdoc ctex即可打开ctex的说明文档.
字体设置 你可以在导言区加入\ctexset控制命令来设置一些字体相关的内容, 例如可以设置摘要和参考文献的名称, 格式如下:
\ctexset{ abstractname = {本文概要}, bibname = {文\quad 献} } ctex设置了一些默认的字体集, 基本上是根据操作系统来设置的, 默认情况下会根据当前操作系统来设置, 这些字体集的具体设置可以在ctex宏包的目录下的. 字体集的设置可以在\documentclass中设置, 也可以在\ctexset中设置, 下面是两种设置方式:
% fontset = &amp;lt;none | adobe | fondol | founder | mac | % macnew | macold | ubuntu | windows | windowsnew | % windowsold |.</description>
    </item>
    
    <item>
      <title>单词</title>
      <link>/post/2020/09/10/%E5%8D%95%E8%AF%8D/</link>
      <pubDate>Thu, 10 Sep 2020 17:00:18 +0800</pubDate>
      
      <guid>/post/2020/09/10/%E5%8D%95%E8%AF%8D/</guid>
      <description>acquire 收购
infrastructure 基础设施
sophistricate 老于世故的
span 跨越
tier [tɪr] 层, 阶, 等级
crash course 速成班
adulation 奉承</description>
    </item>
    
    <item>
      <title>Pandoc&#39;s markdown</title>
      <link>/post/2020/08/27/pandocs-markdown/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/08/27/pandocs-markdown/</guid>
      <description>段落分段必须要用空行隔开，多个空行只当作1个空行。如果需要强制空行，可以用反斜杠符加换行符。例如
段落1段落2段落3段落4 \段落5 \\\段落6段落1 段落2
段落3
段落4
段落5
段落6
扩展名为escaped_line-breaks
标题ATX-style标题标准markdown不要求标题的前面需要空行，而pandoc’s markdown要求在标题前要有空行，标题中可以有url符号和强调符号，例如
# A level-one heading with a [link](/url) and *emphasis*扩展名为blank_before_header
标题属性标题属性中的内容可以是
{#identifier .class .class key=value key=value}扩展为 header_attributes
标题标识符给标题添加一个标识符号，可以自己添加{#identifier}，如果自己不添加，将会自动设为{#标题}。例如：
&amp;lt;!--手动添加--&amp;gt;# My heading {#foo}&amp;lt;!--自动添加--&amp;gt;# 段落跳转到[段落](#段落)跳转到段落
标题编号如果指定了--number-sections，将会自动给各个标题编号，但是有些标题不想被编号，这时，在标题的标识符中添加-或.unnumbered即可，例如：
# My heading {-}# My heading {.unnumbered}代码块缩进代码块只需要4个空格或者一个TAB即可将接下来的内容变为代码块
if (a &amp;gt; 3) {moveShip(5 * gravity, DOWN);}围栏代码块波浪线型可以添加多个属性，例如</description>
    </item>
    
    <item>
      <title>转移到hugo</title>
      <link>/post/2020/08/26/%E8%BD%AC%E7%A7%BB%E5%88%B0hugo/</link>
      <pubDate>Wed, 26 Aug 2020 17:06:33 +0800</pubDate>
      
      <guid>/post/2020/08/26/%E8%BD%AC%E7%A7%BB%E5%88%B0hugo/</guid>
      <description>博客环境搭建 介于hexo框架太麻烦了，要安装nodejs，又要安装各种包，实在是难同步。由于电脑前段时间崩了，重新搭建hexo环境总是失败，实在浪费了很多时间，于是决定迁移到hugo。
在mac搭建hugo环境非常简单，只需要用homebrew即可安装
brew install hugo 这就算安装好hugo了，初始化新的hugo站点则至于要在任意路径下执行，再将主题放到theme中，我这里用的是yihui的hugo-ivy进行的修改，除了yihui的字体好像经过优化了，其他基本类似.
hugo new site &amp;lt;/path/to/site&amp;gt; git clone https://github.com/yihui/hugo-ivy themes/hugo-ivy yihui的主题可以很好的支持行间公式(双美元符$$)，但是不能支持行内公式(单美元符$)，于是从这里中找到解决方案，即在layouts/partials/mathjax.html中加入：
&amp;lt;script type=&amp;#34;text/javascript&amp;#34; async src=&amp;#34;https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;#34;&amp;gt; MathJax.Hub.Config({ tex2jax: { inlineMath: [[&amp;#39;$&amp;#39;,&amp;#39;$&amp;#39;], [&amp;#39;\\(&amp;#39;,&amp;#39;\\)&amp;#39;]], displayMath: [[&amp;#39;$$&amp;#39;,&amp;#39;$$&amp;#39;], [&amp;#39;\[\[&amp;#39;,&amp;#39;\]\]&amp;#39;]], processEscapes: true, processEnvironments: true, skipTags: [&amp;#39;script&amp;#39;, &amp;#39;noscript&amp;#39;, &amp;#39;style&amp;#39;, &amp;#39;textarea&amp;#39;, &amp;#39;pre&amp;#39;], TeX: { equationNumbers: { autoNumber: &amp;#34;AMS&amp;#34; }, extensions: [&amp;#34;AMSmath.js&amp;#34;, &amp;#34;AMSsymbols.js&amp;#34;] } } }); MathJax.Hub.Queue(function() { // Fix &amp;lt;code&amp;gt; tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown.</description>
    </item>
    
    <item>
      <title>数组指针和指针数组</title>
      <link>/post/2020/01/10/%E6%95%B0%E7%BB%84%E6%8C%87%E9%92%88%E5%92%8C%E6%8C%87%E9%92%88%E6%95%B0%E7%BB%84/</link>
      <pubDate>Fri, 10 Jan 2020 09:42:37 +0000</pubDate>
      
      <guid>/post/2020/01/10/%E6%95%B0%E7%BB%84%E6%8C%87%E9%92%88%E5%92%8C%E6%8C%87%E9%92%88%E6%95%B0%E7%BB%84/</guid>
      <description></description>
    </item>
    
    <item>
      <title>(一)数据结构：顺序存储线性表</title>
      <link>/post/2019/12/16/%E4%B8%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E9%A1%BA%E5%BA%8F%E5%AD%98%E5%82%A8%E7%BA%BF%E6%80%A7%E8%A1%A8/</link>
      <pubDate>Mon, 16 Dec 2019 14:23:35 +0000</pubDate>
      
      <guid>/post/2019/12/16/%E4%B8%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E9%A1%BA%E5%BA%8F%E5%AD%98%E5%82%A8%E7%BA%BF%E6%80%A7%E8%A1%A8/</guid>
      <description>顾名思义，顺序存储的线性表就是存储方式是在内存中安装先后顺序依次存储、逻辑结构为线性结构的一种数据结构。 其存储方式如下图所示： 各个储存单元都是顺序排放的。为了顺序存储数据，需要为一个顺序存储线性表指定长度。 下面是顺序存储的线性表结构代码
#define LIST_LENGTH_MAX 20 struct listlinear_t { int data[LIST_LENGTH_MAX]; int length; }listlinear; 为了指示对线性表的操作后的返回状态，可以作如下定义
#define OK 1 #define ERROR 0 #define TRUE 1 #define FALSE 0 线性表初始化 void InitList(listlinear *list) { for(int i=0; i&amp;lt;LIST_LENGTH_MAX; i++) list-&amp;gt;data[i] = 0; list-&amp;gt;length = 0; } 查询数据 查询线性表中的第loc个数据，由于顺序存储的线性表按照先后顺序存储在存储单元内，因此支持随机查询数据, 代码的实现如下
status GetElim(listlinear list, int loc, int *e){ if(list.length == 0 || loc &amp;gt; list.length) return ERROR; *e = list.data[loc]; return OK; } 插入数据 顺序存储的方式使线性表在需要插入数据时要重新排放存储单元的数据，但要在第loc处插入数据时，loc后的数据需要在当前的存储单元后移个位置，如下图，在data[2]处插入数据，则原来data[2]处的数据需要向后存放致data[3]处，后面的数据依次后移。 代码实现如下:</description>
    </item>
    
    <item>
      <title>GIF动图制作</title>
      <link>/post/2019/12/16/gif%E5%8A%A8%E5%9B%BE%E5%88%B6%E4%BD%9C/</link>
      <pubDate>Mon, 16 Dec 2019 11:09:38 +0000</pubDate>
      
      <guid>/post/2019/12/16/gif%E5%8A%A8%E5%9B%BE%E5%88%B6%E4%BD%9C/</guid>
      <description>屏幕录制GIF动图工具 Screen， 缺点是只能在windows系统使用。 LICEcap 同时支持windows和macos </description>
    </item>
    
    <item>
      <title>v2ray&#43;CDN</title>
      <link>/post/2019/12/12/v2ray-cdn/</link>
      <pubDate>Thu, 12 Dec 2019 10:39:51 +0000</pubDate>
      
      <guid>/post/2019/12/12/v2ray-cdn/</guid>
      <description>搭建v2ray并使用CDN的方法，拯救被墙的IP，CDN + v2ray，安全的科学上网方法 </description>
    </item>
    
    <item>
      <title>skip、merge、AMVP模式</title>
      <link>/post/2019/12/10/skipmergeamvp%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Tue, 10 Dec 2019 19:09:17 +0000</pubDate>
      
      <guid>/post/2019/12/10/skipmergeamvp%E6%A8%A1%E5%BC%8F/</guid>
      <description>在H.265中，帧间预测模式包括Skip模式、Merge模式、AMVP模式，其中，Skip模式是一种特殊的Merge模式。它们所需要编码的信息如下
MVP MVD 量化残差 Skip ✔ ❌ ❌ Merge ✔ ❌ ✔ AMVP ✔ ✔ ✔ MVP的获取方法都是通过在编解码端建立候选列表，但要注意的是Skip模式和Merge模式建立MVP列表的方法一样，而AMVP建立候选列表的方式与Merge模式建立候选列表的方式不一样</description>
    </item>
    
    <item>
      <title>梯度下降法原理</title>
      <link>/post/2019/12/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%8E%9F%E7%90%86/</link>
      <pubDate>Tue, 10 Dec 2019 09:40:39 +0000</pubDate>
      
      <guid>/post/2019/12/10/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%8E%9F%E7%90%86/</guid>
      <description>在机器学习中，梯度下降法是首先接触的到的最优求解方法，特别是在线性回归中，目标函数往往是一个凸函数(convex)，可以通过梯度下降法求得全局最优解。梯度下降法的公式比较简单。例如，优化函数$J(\theta)$的梯度为$\nabla J(\theta)$，那么其梯度下降过程为：
$$ \theta^{(k+1)} = \theta^{(k)}-\eta \nabla J(\theta)|_{\theta = \theta^{(k)}} $$
也可以写成： $$ \theta_i^{(k+1)} = \theta_i^{(k)}-\eta \frac{\partial J(\theta_i)}{\partial \theta_i}|_{\theta_i = \theta_i^{(k)}} $$ 梯度下降法的推导方法如下：
假设待优化函数为$J(\theta)$，根据一阶泰勒展开，得到 $$ J(\theta) = J(\theta^{(k)})+\nabla J(\theta^{(k)})(\theta-\theta^{(k)}) $$ 假设当前位置$\theta^{(k)}$的下一步更新位置为$\theta^{(k+1)}$，那么 $$ J(\theta^{(k+1)}) = J(\theta^{(k)})+\nabla J(\theta^{(k)})(\theta^{(k+1)}-\theta^{(k)}) $$ 也就是： $$ J(\theta^{(k+1)})-J(\theta^{(k)})=\nabla J(\theta^{(k)})(\theta^{(k+1)}-\theta^{(k)}) $$ 我们的目标是求的使$J(\theta)$的值取得最小的$\theta$，那么每次更新应该保证$J(\theta^{(k+1)})-J(\theta^{(k)})&amp;lt;0$，并且越小越好。可以观察上式中$\nabla J(\theta^{(k)})$和$\theta^{(k+1)}-\theta^{(k)}$表示的是两个向量的乘积。 $$ \nabla J(\theta^{(k)})(\theta^{(k+1)}-\theta^{(k)})=||\nabla J(\theta^{(k)})||\cdot||(\theta^{(k+1)}-\theta^{(k)}||\cdot cos\alpha $$ 回顾向量的乘法，$J(\theta^{(k+1)})-J(\theta^{(k)})$的值取决于$||\nabla J(\theta^{(k)})||$,$||(\theta^{(k+1)}-\theta^{(k)}||$以及$cos\alpha$的大小，对于给定目标函数$J(\theta)$，结果只与$\theta^{(k+1)}$和与之相关的$cos\alpha$如何取值有关。当$||(\theta^{(k+1)}-\theta^{(k)}||$确定时，可使$cos\alpha=-1$来使得向量乘积负得最大（即值最小），也就是两个向量的夹角为$180^{\circ}$，此时有 $$ \frac{(\theta^{(k+1)}-\theta^{(k)})}{||(\theta^{(k+1)}-\theta^{(k)})||}=-\frac{\nabla J(\theta^{(k)})}{||\nabla J(\theta^{(k)})||} $$ 也就是: $$ \theta^{(k+1)} = \theta^{(k)} -\frac{||(\theta^{(k+1)}-\theta^{(k)})||}{||\nabla J(\theta^{(k)})||} \nabla J(\theta^{(k)}) $$ 令$\eta=\frac{||(\theta^{(k+1)}-\theta^{(k)})||}{||\nabla J(\theta^{(k)})||}$，上式可写成: $$ \theta^{(k+1)} = \theta^{(k)} -\eta\nabla J(\theta^{(k)}) $$ 或写成： $$ \theta_i^{(k+1)} = \theta_i^{(k)} -\eta\frac{\partial J(\theta_i)}{\partial\theta_i} |_{\theta_i=\theta_i^{(k)}} $$</description>
    </item>
    
    <item>
      <title>YCbCr格式的理解</title>
      <link>/post/2019/12/05/ycbcr%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <pubDate>Thu, 05 Dec 2019 16:37:21 +0000</pubDate>
      
      <guid>/post/2019/12/05/ycbcr%E6%A0%BC%E5%BC%8F%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <description>YCbCr格式的理解 YUV格式 YUV格式有别于传统的RGB三原色格式，YUV是将亮度分量Y和两个色度分量U、V分开，其中Y表示灰阶值，U表示蓝色与亮度值之间的差异，V表示红色与亮度之间的差异，于是对亮度分量的处理Y并不影响U、V分量，此外单独传输Y分量可向后兼容老式黑白电视。YCbCr格式与YUV格式类似，YCbCr主要应用于图像、视频压缩的数字彩色信息表示，是YUV压缩和偏移的版本。RGB与YCbCr的转换如下：
$$ \begin{bmatrix} Y \\ Cb \\ Cr \end{bmatrix} = \begin{bmatrix} 0.299 &amp;amp; 0.587&amp;amp; 0.114 \\ -0.169&amp;amp; -0.331&amp;amp; 0.449 \\ 0.449 &amp;amp; -0.418&amp;amp; -0.0813 \\ \end{bmatrix} \begin{bmatrix} R \\ G \\ B \end{bmatrix}+ \begin{bmatrix} 0 \\ 128 \\ 128 \end{bmatrix} $$
$$ \begin{bmatrix} R \\ G \\ B \end{bmatrix} = \begin{bmatrix} 1.0 &amp;amp; 0.0 &amp;amp; 1.402 \\ 1.0 &amp;amp; -0.344 &amp;amp; -0.714 \\ 1.0 &amp;amp; 1.772 &amp;amp; 0.0 \end{bmatrix} \begin{bmatrix} Y \\ Cb-128 \\ Cr-128 \end{bmatrix} $$</description>
    </item>
    
    <item>
      <title>cmake编译与测试opencv4</title>
      <link>/post/2019/11/28/cmake%E7%BC%96%E8%AF%91%E4%B8%8E%E6%B5%8B%E8%AF%95opencv4/</link>
      <pubDate>Thu, 28 Nov 2019 15:43:42 +0000</pubDate>
      
      <guid>/post/2019/11/28/cmake%E7%BC%96%E8%AF%91%E4%B8%8E%E6%B5%8B%E8%AF%95opencv4/</guid>
      <description>保证已经安装：
GCC 4.4.x or later CMake 2.8.7 or higher Git GTK+2.x or higher, incuding headers(libgtk2.0-dev) pkg-config Python 2.6 or later and Numpy 1.5 or later with developer package (python-dev, python-numpy) ffmpeg or libav development packages: libavcodec-dev, libavformat-dev, libswscale-dev [optional] libtbb2 libtbb-dev [optional] libdc1394 2.x [optional] libjpeg-dev, libpng-dev, libtiff-dev, libjasper-dev, libdc1394-22-dev [optional] CUDA Toolkit 6.5 or higher 可通过以下命令安装 # [compiler] sudo apt-get install build-essential # required sudo apt-get install cmake git libgtk2.</description>
    </item>
    
    <item>
      <title>交叉编译opencv并移植至树莓派</title>
      <link>/post/2019/11/22/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91opencv%E5%B9%B6%E7%A7%BB%E6%A4%8D%E8%87%B3%E6%A0%91%E8%8E%93%E6%B4%BE/</link>
      <pubDate>Fri, 22 Nov 2019 11:14:26 +0000</pubDate>
      
      <guid>/post/2019/11/22/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91opencv%E5%B9%B6%E7%A7%BB%E6%A4%8D%E8%87%B3%E6%A0%91%E8%8E%93%E6%B4%BE/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LK光流法</title>
      <link>/post/2019/11/18/lk%E5%85%89%E6%B5%81%E6%B3%95/</link>
      <pubDate>Mon, 18 Nov 2019 11:10:53 +0000</pubDate>
      
      <guid>/post/2019/11/18/lk%E5%85%89%E6%B5%81%E6%B3%95/</guid>
      <description>1 超定方程组求解 1.1 超定方程组 超定方程组是指方程的个数大于未知数个数的方程组，例如
$$ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; \\ a_{n1} &amp;amp; a_{n2} &amp;amp; a_{n3} \end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ &amp;hellip; \\ b_n \\ \end{bmatrix} $$
其中，$n&amp;gt;3$，将上述方程表示为$\boldsymbol{A}_{n\times 3}x_{3\times 1}=b_{n\times 1}$，该方程不能按照一般的求解方法求解，但可以求的最小二乘法的解。
先给出结论，该超定方程组的最小二乘解为： $$x=(\boldsymbol{A}^{T}\boldsymbol{A})^{-1}\boldsymbol{A}^{T}b$$
1.2 超定方程组的最小二乘解 对于无一般解的超定方程组$\boldsymbol{A}x=b$来说，假设$r=\boldsymbol{A}x -b$，使得$||r||^2_2=（\boldsymbol{A}x -b)^{T}(\boldsymbol{A}x -b)$的值最小的解即为最小二乘解。于是，问题转变为最小化$(\boldsymbol{A}x -b)^{T}(\boldsymbol{A}x -b)$，令 $$J(x)=(\boldsymbol{A}x-b)^T(\boldsymbol{A}x-b)$$ 对上式求导，可得
$$ \begin{aligned} \frac{\partial J(x)}{\partial x} &amp;amp; =\frac{\partial{(\boldsymbol{A}x-b)^T(\boldsymbol{A}x-b)}}{\partial{x}} \\ &amp;amp; =\frac{\partial(x^T\boldsymbol{A}^T\boldsymbol{A}x-x^T\boldsymbol{A}^Tb-b^T\boldsymbol{A}x+b^Tb)}{\partial{x}} \\ &amp;amp; =(\boldsymbol{A}^T\boldsymbol{A}x)^T+x^T\boldsymbol{A}\boldsymbol{A}-b^T\boldsymbol{A}-b^T\boldsymbol{A} \\ &amp;amp; =2(x^T\boldsymbol{A}^T\boldsymbol{A}-b^T\boldsymbol{A}) \end{aligned} $$</description>
    </item>
    
  </channel>
</rss>
