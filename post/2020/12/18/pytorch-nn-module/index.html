<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Pytorch nn-Module - excelkks</title>
    <meta property="og:title" content="Pytorch nn-Module - excelkks">
    
    <meta name="twitter:card" content="summary">

    
      
    

    
      
      <meta property="description" content="[toc]
[&amp;hellip;] 在nn(neural network)模块中，有一个nn.Module类，这是一个快速构建神经网络的基类。你可以通过继承这个类，并重写forward(input)方法，来创建一个自定义各个网络层的神经网络类。例如下面这个Net	类定义了一个LeNet-5网络。
[&amp;hellip;] import torch import torch.nn as nn import &amp;hellip;">
      <meta property="og:description" content="[toc]
[&amp;hellip;] 在nn(neural network)模块中，有一个nn.Module类，这是一个快速构建神经网络的基类。你可以通过继承这个类，并重写forward(input)方法，来创建一个自定义各个网络层的神经网络类。例如下面这个Net	类定义了一个LeNet-5网络。
[&amp;hellip;] import torch import torch.nn as nn import &amp;hellip;">
      
    

    
    

    

    
    


<link href='//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/custom.css" />
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script>

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>

<script>
  (function(d) {
    var config = {
      kitId: 'hcr2dmn',
      scriptTimeout: 3000,
      async: true
    },
    h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)
  })(document);
</script>
<link rel="icon" type="image/png" href="/images/favicon/favicon.png" />
<link rel="shortcut icon" type="image/png" href="/images/favicon/favicon.png" />

  </head>

  
  <body class="post">
    <header class="masthead">
      <h1><a href="/"><img src="/images/favicon/favicon.png" /></a></h1>

<p class="tagline">just do it and waiting</p>

      <nav class="menu">
  <input id="menu-check" type="checkbox" hidden/>
  <label id="menu-label" for="menu-check" class="unselectable" hidden>
    <span class="icon close-icon">✕</span>
    <span class="icon open-icon">☰</span>
    <span class="text">Menu</span>
  </label>
  <ul>
  
  
  <li><a href="/">主页</a></li>
  
  <li><a href="/about/">关于</a></li>
  
  <li><a href="/categories/">分类</a></li>
  
  <li><a href="/tags/">标签</a></li>
  
  
  </ul>
</nav>

    </header>

    <article class="main">
      <header class="title">
      
<h1>Pytorch nn-Module</h1>

<h3>
  2020-12-18</h3>
<hr>


      </header>



<p>[toc]</p>
<h2 id="nnmodule"><code>nn.Module</code></h2>
<h3 id="继承nnmodule类">继承<code>nn.Module</code>类</h3>
<p>在nn(neural network)模块中，有一个<code>nn.Module</code>类，这是一个快速构建神经网络的基类。你可以通过继承这个类，并重写<code>forward(input)</code>方法，来创建一个自定义各个网络层的神经网络类。例如下面这个<code>Net</code>	类定义了一个LeNet-5网络。</p>
<p><img src="/images/pytorch/LeNet-5.png" alt="LeNet-5"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self):
        super(Net, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># 1 input image channel, 6 output channels, 3x3 square convolution</span>
        <span style="color:#75715e"># kernel</span>
        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">3</span>)
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>)
        <span style="color:#75715e"># an affine operation: y = Wx + b</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">120</span>)  <span style="color:#75715e"># 6*6 from image dimension</span>
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">120</span>, <span style="color:#ae81ff">84</span>)
        self<span style="color:#f92672">.</span>fc3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">84</span>, <span style="color:#ae81ff">10</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#75715e"># Max pooling over a (2, 2) window</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x)), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>))
        <span style="color:#75715e"># If the size is a square you can only specify a single number</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x)), <span style="color:#ae81ff">2</span>)
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>num_flat_features(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc2(x))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc3(x)
        <span style="color:#66d9ef">return</span> x

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">num_flat_features</span>(self, x):
        size <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">1</span>:]  <span style="color:#75715e"># all dimensions except the batch dimension</span>
        num_features <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
        <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> size:
            num_features <span style="color:#f92672">*=</span> s
        <span style="color:#66d9ef">return</span> num_features


net <span style="color:#f92672">=</span> Net()
</code></pre></div><p>上面用到的<code>nn.Conv2d</code>计算的对象是4维的，维度依次为(nSamples, nChannels, Height, Width)，也就是input需要按照这个顺序排列。如果传入的是单个Sample，可以通过<code>input.unsqueeze(0)</code>增加一个sample维度。<a href="#%60unsqueeze%60">参考<code>unsqueeze</code></a></p>
<h3 id="nnmodule继承类的调用"><code>nn.Module</code>继承类的调用</h3>
<p><code>nn.Module</code>类通过方法<code>__call__()</code>来使类的以函数调用方式返回<code>forward</code>的计算结果，为了说明，下面是<code>nn.Module</code>有关的最简定义：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Module</span>(object):
  <span style="color:#66d9ef">def</span> __call__(self, <span style="color:#f92672">*</span>input, <span style="color:#f92672">**</span>kargs):
    <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>forward(<span style="color:#f92672">*</span>input, <span style="color:#f92672">**</span>kargs)
</code></pre></div><p>因此，当执行<code>out = net(input)</code>时，实际上返回的是<code>forward</code>方法的返回值，是一个带<code>grad_fn</code>属性的<code>tensor</code>。</p>
<h3 id="parameters方法"><code>parameters()</code>方法</h3>
<p><code>parameters()</code>方法可以返回遍历这个网络的所有参数的迭代器，一般使用<code>for</code>来遍历。例如:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> net<span style="color:#f92672">.</span>parameters():
  print(param<span style="color:#f92672">.</span>size())

<span style="color:#75715e"># &gt;&gt; torch.Size([6, 1, 3, 3])</span>
<span style="color:#75715e"># &gt;&gt; torch.Size([6, 16, 3, 3])</span>
<span style="color:#75715e"># &gt;&gt; ...</span>
</code></pre></div><h3 id="zero_grad方法"><code>zero_grad()</code>方法</h3>
<p>net的反向传播过程的梯度累加的，所以每次<code>backward()</code>都会将新的梯度累加到<code>grad</code>中，因此，除非必要，我们应该在每次反向传播前都进行一次梯度清0，也就是调用</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># 清除网络中所有参数的梯度</span>
net<span style="color:#f92672">.</span>zero_grad()
</code></pre></div><h3 id="loss-function">loss function</h3>
<p>neural network的损失函数可以参考<a href="https://pytorch.org/docs/stable/nn.html#loss-functions">这里</a>，以MSE损失函数为例，过程只需要分两步，1. 定义损失函数， 2. 调用损失函数。例如</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">output <span style="color:#f92672">=</span> net(input)
target <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">10</span>)
target <span style="color:#f92672">=</span> target<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss()

loss <span style="color:#f92672">=</span> criterion(output, target)
</code></pre></div><h3 id="更新参数">更新参数</h3>
<p>更新参数的方法可以手动更新，也可以通过内置更新模块来更新</p>
<ul>
<li>
<p>手动更新</p>
<p>以SGD为例，遍历网络中的每个参数，并更新，示例如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
<span style="color:#66d9ef">for</span> f <span style="color:#f92672">in</span> net<span style="color:#f92672">.</span>parameters():
  f<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>sub_(f<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data <span style="color:#f92672">*</span> learning_rate)
</code></pre></div></li>
<li>
<p>优化package</p>
<p>torch的优化package包含多种定义好的优化方法，使用方法也很简单，1. 定义优化方法，2. 调用优化方法<code>step()</code>进行一次更新。例如</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(net<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>)
optimizer<span style="color:#f92672">.</span>zero_grad()  <span style="color:#75715e"># 与nn.Module中的nn.Module.zero_grad() 一样</span>
output <span style="color:#f92672">=</span> net(input)
loss <span style="color:#f92672">=</span> criterion(output, target)
loss<span style="color:#f92672">.</span>backward()
optimizer<span style="color:#f92672">.</span>step()
</code></pre></div></li>
</ul>
<h2 id="一个典型的nnmodule神经网络的训练过程">一个典型的<code>nn.Module</code>神经网络的训练过程</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
<span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim

<span style="color:#75715e"># ----------------- 定义网络 ----------------------</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self):
        super(Net, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>)
        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">5</span>)
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">120</span>)
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">120</span>, <span style="color:#ae81ff">84</span>)
        self<span style="color:#f92672">.</span>fc3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">84</span>, <span style="color:#ae81ff">10</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x)))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x)))
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>)
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc2(x))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc3(x)
        <span style="color:#66d9ef">return</span> x


net <span style="color:#f92672">=</span> Net()

<span style="color:#75715e"># ---------------- 定义损失函数和优化器 ------------------</span>

criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(net<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>, momentum<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>)

<span style="color:#75715e"># ----------------  训练过程 ---------------------</span>
<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>):  <span style="color:#75715e"># loop over the dataset multiple times</span>

    running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    <span style="color:#66d9ef">for</span> i, data <span style="color:#f92672">in</span> enumerate(trainloader, <span style="color:#ae81ff">0</span>): <span style="color:#75715e"># traninloader 中包含了训练数据集，这里未展示</span>
        <span style="color:#75715e"># get the inputs; data is a list of [inputs, labels]</span>
        inputs, labels <span style="color:#f92672">=</span> data

        <span style="color:#75715e"># zero the parameter gradients</span>
        optimizer<span style="color:#f92672">.</span>zero_grad()

        <span style="color:#75715e"># forward + backward + optimize</span>
        outputs <span style="color:#f92672">=</span> net(inputs)
        loss <span style="color:#f92672">=</span> criterion(outputs, labels)
        loss<span style="color:#f92672">.</span>backward()
        optimizer<span style="color:#f92672">.</span>step()

        <span style="color:#75715e"># print statistics</span>
        running_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">2000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">1999</span>:    <span style="color:#75715e"># print every 2000 mini-batches</span>
            print(<span style="color:#e6db74">&#39;[</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">, </span><span style="color:#e6db74">%5d</span><span style="color:#e6db74">] loss: </span><span style="color:#e6db74">%.3f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span>
                  (epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, running_loss <span style="color:#f92672">/</span> <span style="color:#ae81ff">2000</span>))
            running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>

print(<span style="color:#e6db74">&#39;Finished Training&#39;</span>)

</code></pre></div><h2 id="在gpu上训练">在GPU上训练</h2>
<p>定义一个GPU设备，然后将网络和训练集和lable都放入该设备中</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda:0&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
net<span style="color:#f92672">.</span>to(device)  <span style="color:#75715e"># nn.Module.to</span>
input, lables <span style="color:#f92672">=</span> data[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>to(device), data[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># torch.Tensor.to</span>
</code></pre></div><h2 id="unsqueeze"><code>unsqueeze</code></h2>
<p><code>unsqueeze()</code>是指在扩展指定维度，例如</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>]) <span style="color:#75715e"># shape为(3)</span>
xx <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>) <span style="color:#75715e"># 在第0个维度扩张，xx为 [[1,2,3]], shape 为(1,3)</span>
xxx <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 在第1个维度扩张，xxx 为[[1], [2], [3]], shape为(3, 1)</span>
</code></pre></div>

  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/post/2020/12/18/pytorch%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/">Pytorch自定义函数</a></span>
  <span class="nav-next"><a href="/post/2020/12/18/pytorch-sequential/">Pytorch Sequential</a> &rarr;</span>
</nav>





<script src="//yihui.org/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.org/js/center-img.js"></script>

  



<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/tex.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/c.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/c&#43;&#43;.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/shell.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/markdown.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



  
  <hr>
  <div class="copyright">© <a href="https://excelkks.github.io">KKS</a> 2020 | <a href="https://github.com/excelkks">Github</a> | <a href="mailto:excelkks@hotmail.com">E-Mail</a></div>
  
  </footer>
  </article>
  
  </body>
</html>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

