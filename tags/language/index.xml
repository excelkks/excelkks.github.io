<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>language on excelkks</title>
    <link>/tags/language/</link>
    <description>Recent content in language on excelkks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Dec 2020 11:39:33 +0800</lastBuildDate><atom:link href="/tags/language/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pytorch Sequential</title>
      <link>/post/2020/12/18/pytorch-sequential/</link>
      <pubDate>Fri, 18 Dec 2020 11:39:33 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch-sequential/</guid>
      <description>torch.nn.Sequential 快速构建一个neural network可以通过torch.nn.Module构建类，也可通过设计一个torch.nn.Sequential来构建网络计算过程。实际上，只需要将网络上的各个计算过程都堆叠在Sequential中既可以了，例如：
model = torch.nn.Sequential(  torch.nn.Linear(3, 1),  torch.nn.Flatten(0,1)) Sequential是一个容器，一般来说，可以用多个Sequential来构建Module. 例如LeNet-5网络的构建
class LeNet5(torch.nn.Module):  def __init__(self):  super(LeNet5,self).__init__()  self.conv1 = torch.nn.Sequential(  torch.nn.Conv2d(1,6,5),  torch.nn.ReLU(),  torch.nn.MaxPool2d(2,2)  )  self.conv2 = torch.nn.Sequential(  torch.nn.Conv2d(6,16,5),  torch.nn.ReLU(),  torch.nn.MaxPool2d(2,2)  )  self.conv3 = torch.nn.Sequential(  torch.nn.Conv2d(16,120,5),  torch.nn.ReLU()  )  self.fc = torch.nn.Sequential(  torch.nn.Linear(120,84),  torch.nn.ReLU(),  torch.nn.Linear(84,10),  torch.nn.LogSoftmax(dim=-1)  )  def forward(self, image):  output = self.</description>
    </item>
    
    <item>
      <title>Pytorch nn-Module</title>
      <link>/post/2020/12/18/pytorch-nn-module/</link>
      <pubDate>Fri, 18 Dec 2020 11:35:09 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch-nn-module/</guid>
      <description>[toc]
nn.Module 继承nn.Module类 在nn(neural network)模块中，有一个nn.Module类，这是一个快速构建神经网络的基类。你可以通过继承这个类，并重写forward(input)方法，来创建一个自定义各个网络层的神经网络类。例如下面这个Net	类定义了一个LeNet-5网络。
import torch import torch.nn as nn import torch.nn.functional as F   class Net(nn.Module):   def __init__(self):  super(Net, self).__init__()  # 1 input image channel, 6 output channels, 3x3 square convolution  # kernel  self.conv1 = nn.Conv2d(1, 6, 3)  self.conv2 = nn.Conv2d(6, 16, 3)  # an affine operation: y = Wx + b  self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6*6 from image dimension  self.</description>
    </item>
    
    <item>
      <title>Pytorch自定义函数</title>
      <link>/post/2020/12/18/pytorch%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/</link>
      <pubDate>Fri, 18 Dec 2020 11:33:36 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/</guid>
      <description>[toc]
torch.autograd.Function 自定义一个计算过程，并且定义其反向传播过程。例如计算 $y = a+bx+cx^2+dx^3$时，用两步替代该过程 $y= a+b\times P_3(c+dx), P_3(x) = \frac{1}{2}(5x^3-3x)$
那么可以通过torch.autograd.Function定义$P_3(x)$，过程如下：
# -*- coding: utf-8 -*- import torch import math   class LegendrePolynomial3(torch.autograd.Function):  &amp;#34;&amp;#34;&amp;#34; We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. &amp;#34;&amp;#34;&amp;#34;   @staticmethod  def forward(ctx, input):  &amp;#34;&amp;#34;&amp;#34; In the forward pass we receive a Tensor containing the input and return a Tensor containing the output.</description>
    </item>
    
    <item>
      <title>Pytorch基础</title>
      <link>/post/2020/12/18/pytorch%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Fri, 18 Dec 2020 11:30:07 +0800</pubDate>
      
      <guid>/post/2020/12/18/pytorch%E5%9F%BA%E7%A1%80/</guid>
      <description>[toc]
Tensor的创建 tensor是基本的数据类型，基本的创建方式有
 torch.empty(5, 3) 未初始化的tensor, 里面的值为内存中原有的值 torch.randn(5,3) 创建tensor，并随机初始化 torch.zeros(5, 3, dtype=torch.long) 创建tensor，并初始化为0，指定数据类型 torch.tensor([5.5, 3])创建tensor，并直接值初始化  除了上述的创建方式，还有可以利用已经存在的tensor来创建，假设现有x = torch.zeros(5,3, dtype=torch.float32)，那么，可以根据以下方式创建
 x.new_ones(5,3) 创建tensor，并值初始化为0，除了shape外，其他属性与x一致 torch.randn_like(x, dtype=torch.float) 创建tensor，随机初始化值，除了明确指定的属性type=torch.float外，其他属性与x一致  Operations 加法   直接加
x = torch.randn(5, 3) y = torch.randn(5, 3) # 直接加 result1 = x + y   函数方法
# 结果赋值到result2 result2 = torch.add(x, y) # 预先指定结果保存到变量 torch.add(x, y, out=result3)   立地加(in-place)
# 立地(in-place)加，结果直接加到被加数中, 一般立地加的函数都以_结束， # 例如x.copy_(y), x.</description>
    </item>
    
    <item>
      <title>Tensorflow</title>
      <link>/post/2020/12/09/tensorflow/</link>
      <pubDate>Wed, 09 Dec 2020 09:58:47 +0800</pubDate>
      
      <guid>/post/2020/12/09/tensorflow/</guid>
      <description>使用GPU 为了避免占用全部GPU，需要加上
import os os.environ[&amp;#39;CUDA_DEVICES_ORDER&amp;#39;] = &amp;#39;PCI_BUS_ID&amp;#39; os.environ[&amp;#39;CUDA_VISIBLE_DEVICES&amp;#39;] = &amp;#39;0&amp;#39; config = tf.ConfigProto() #config.gpu_options.per_process_gpu_memory_fraction = 0.9 config.gpu_options.allow_growth = True tf.truncated_normal_initializer
生成截断正态分布的初始化器类
 参数  mean: 分布的均值 stddev: standard deviation，标准差 seed: 随机种子 dtype: 数据类型，只能是float的类型，可选位数    tf.get_variable()
获取一个变量或者创建一个新的变量
 重要参数  shape: list类型，是指变量的shape Initializer: 用于生成变量的初始化器    tf.nn.conv2d()
2维卷积函数，列举主要参数tf.nn.conv2d(input, filter=None, strides=None, paddig=None, data_format=&amp;lsquo;NHWC&amp;rsquo;)。
 重要参数  input: 输入4-D的Tensor，数据格式只能是half,bfloat16,float32,float64 filter: 卷积核，也是一个4-D tensor，数据格式必须与input一样 strides: 移动步长，可以是长度为1,2,4的list，长度为1时，表示宽高方向的移动步长，并且一致。长度为2时，分别指定宽高方向的移动步长。长度为4时，设定根据data_format指定的位置对应方向移动步长。 padding: &amp;lsquo;VALID&amp;rsquo;表示不填充， SAME&amp;rsquo;表示填充。填充方式根据filter而定，如果filter的一个维度上的长度为奇数，则在该维度起始处和结束处填充0，如果为偶数则在结束处填充0。    一般来说，input的各个维度为[batch, in_height, in_width, in_channels]，filter的各个维度为 [filter_height, filter_width, in_channels, out_channels]</description>
    </item>
    
  </channel>
</rss>
